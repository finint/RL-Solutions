## 第六章 时序差分学习

本章介绍了时序差分（TD）学习，这是一种与蒙特卡洛（MC）方法完全不同的学习方法，其思想是直接从后一个猜测中学习前一个猜测。

在预测问题中，TD(0) 算法自然地运用了一种在线的、完全递增的方法实现，不必等到一幕的结束。TD(0) 算法已被证明能够收敛到价值函数 $v_{\pi}$，但没有理论证明它与 MC 方法的收敛速度哪个更快，但在实践中，TD 方法通常比常量 $\alpha$ MC 方法收敛得更快。此外，TD 方法可以批量更新，只要步长 $\alpha$ 足够小，TD(0) 就能收敛到与 $\alpha$ 无关的唯一结果。

在控制问题中，同轨策略的 Sarsa 算法与离轨策略的 Q 学习方法都能够解决这类问题。基于这两种方法，期望 Sarsa 遵循 Q 学习的模式，向期望意义上的 Sarsa 算法所决定的方向上移动。上述方法存在最大化偏差的问题，因此又提出了双学习，利用两个 Q 函数，其中一个用来确定动作，另一个用来计算其价值的估计。

## 本节目录
1. [习题解答](习题解答.md)
2. [代码案例](代码案例.md)